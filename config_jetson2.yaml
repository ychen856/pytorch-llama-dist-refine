general:
  config_type: config.yaml
  temperature: 0.6
  top_p: 0.9
  max_seq_len: 20
  max_gen_len: 64
  max_batch_size: 4
  nsamples: 128
  Hyper_m: 5
  Lamda: 0.08
  sparsity_ratio: 0.7
  use_variant: false
  splitting_point: 21

model_config:
  attention_bias: false
  attention_dropout: 0.0
  bos_token_id: 1
  eos_token_id: 2
  hidden_act: "silu"
  hidden_size: 4096
  initializer_range: 0.02
  intermediate_size: 11008
  max_position_embeddings: 2048
  model_type: "llama"
  num_attention_heads: 32
  num_hidden_layers: 32
  num_key_value_heads: 32
  pad_token_id: 0
  pretraining_tp: 1
  rms_norm_eps: 1e-05
  rope_scaling: null
  rope_theta: 10000.0
  tie_word_embeddings: false
  torch_dtype: "float16"
  transformers_version: "4.37.2"
  use_cache: true
  vocab_size: 32000

edge_layer_setting:
  max_layers: 4
  start_idx: 0
  end_idx_buff: 3
  end_idx:  2



llama_2_7b_hf:
  #ckpt_dir_hf: /home/yichun/workspace/llama-2-7b-chat-hf/
  #tokenizer_path_hf: /home/yichun/workspace/llama-2-7b-chat-hf/
  ckpt_dir_hf: /home/li/ychen_storage/workspace/llama-2-7b-chat-hf
  tokenizer_path_hf: /home/li/ychen_storage/workspace/llama-2-7b-chat-hf/tokenizer.model


llama_2_7b_sep:
    ckpt_dir_hf_sep: /home/li/ychen_storage/workspace/llama-2-7b-chat-hf-sep
    tokenizer_path_hf_sep: /home/li/ychen_storage/workspace/llama-2-7b-chat-hf-sep/tokenizer.model

communication:
    #nrp server
    server_ip: test-server-service.nrp-nautilus.io
    server_port: 80

    #server_ip: 104.171.203.49
    #server_port: 8000

    #--gateway_ip: 10.7.48.14
    #gateway_ip: 192.168.0.136

    # lab pc
    #gateway_ip: 10.147.17.35
    #gateway_port: 8000

    #lab server
    #gateway_ip: 10.7.48.68
    #gateway_port: 8000

    #nrp edge server
    #gateway_ip: test-edge-server-service.nrp-nautilus.io
    #gateway_port: 80


